{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/amirimmd/Machine_Learnin/blob/main/NN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **1**\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "63jjQzIl-Hjf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class NeuralNetwork:\n",
        "    def __init__(self, layer_sizes, activation_function='sigmoid', learning_rate=0.01):\n",
        "        self.layer_sizes = layer_sizes\n",
        "        self.num_layers = len(layer_sizes)\n",
        "        self.weights = [np.random.randn(layer_sizes[i], layer_sizes[i-1]) for i in range(1, self.num_layers)]\n",
        "        self.biases = [np.zeros((layer_sizes[i], 1)) for i in range(1, self.num_layers)]\n",
        "        self.activation_function = activation_function\n",
        "        self.learning_rate = learning_rate\n",
        "\n",
        "    def _sigmoid(self, z):\n",
        "        return 1 / (1 + np.exp(-z))\n",
        "\n",
        "    def _sigmoid_derivative(self, z):\n",
        "        return self._sigmoid(z) * (1 - self._sigmoid(z))\n",
        "\n",
        "    def _relu(self, z):\n",
        "        return np.maximum(0, z)\n",
        "\n",
        "    def _relu_derivative(self, z):\n",
        "        return np.where(z > 0, 1, 0)\n",
        "\n",
        "    def _forward_propagation(self, X):\n",
        "        activations = [X]\n",
        "        weighted_sums = []\n",
        "\n",
        "        for i in range(self.num_layers - 1):\n",
        "            weighted_sum = np.dot(self.weights[i], activations[i]) + self.biases[i]\n",
        "            weighted_sums.append(weighted_sum)\n",
        "\n",
        "            if self.activation_function == 'sigmoid':\n",
        "                activations.append(self._sigmoid(weighted_sum))\n",
        "            elif self.activation_function == 'relu':\n",
        "                activations.append(self._relu(weighted_sum))\n",
        "\n",
        "        return activations, weighted_sums\n",
        "\n",
        "    def _backward_propagation(self, X, y, activations, weighted_sums):\n",
        "        m = X.shape[1]\n",
        "        delta = (activations[-1] - y) * self._sigmoid_derivative(weighted_sums[-1])\n",
        "\n",
        "        biases_gradients = [np.sum(delta, axis=1, keepdims=True) / m]\n",
        "        weights_gradients = [np.dot(delta, activations[-2].T) / m]\n",
        "\n",
        "        for i in range(2, self.num_layers):\n",
        "            delta = np.dot(self.weights[-i+1].T, delta) * \\\n",
        "                    (self._sigmoid_derivative(weighted_sums[-i]) if self.activation_function == 'sigmoid' else self._relu_derivative(weighted_sums[-i]))\n",
        "\n",
        "            biases_gradients.insert(0, np.sum(delta, axis=1, keepdims=True) / m)\n",
        "            weights_gradients.insert(0, np.dot(delta, activations[-i-1].T) / m)\n",
        "\n",
        "        return biases_gradients, weights_gradients\n",
        "\n",
        "    def train(self, X, y, epochs=1000):\n",
        "        for epoch in range(epochs):\n",
        "            activations, weighted_sums = self._forward_propagation(X)\n",
        "\n",
        "            biases_gradients, weights_gradients = self._backward_propagation(X, y, activations, weighted_sums)\n",
        "\n",
        "            for i in range(self.num_layers - 1):\n",
        "                self.weights[i] -= self.learning_rate * weights_gradients[i]\n",
        "                self.biases[i] -= self.learning_rate * biases_gradients[i]\n",
        "\n",
        "    def predict(self, X):\n",
        "        activations, _ = self._forward_propagation(X)\n",
        "        return activations[-1]\n",
        "\n",
        "# Determining the network architecture (number of nodes in each layer)\n",
        "layer_sizes = [2, 4, 1]\n",
        "\n",
        "# Building a network with a specified number of layers, activity function and learning rate\n",
        "nn = NeuralNetwork(layer_sizes, activation_function='sigmoid', learning_rate=0.1)\n",
        "\n",
        "# Generating synthetic input and output data\n",
        "X_train = np.array([[0, 0], [0, 1], [1, 0], [1, 1]]).T\n",
        "y_train = np.array([[0, 1, 1, 0]])\n",
        "\n",
        "# Network training with input and output data\n",
        "nn.train(X_train, y_train, epochs=10000)\n",
        "\n",
        "# Output prediction for new inputs\n",
        "X_new = np.array([[0, 1], [1, 1]]).T\n",
        "predictions = nn.predict(X_new)\n",
        "\n",
        "print(\"Predictions:\", predictions)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i32hKwel1OZS",
        "outputId": "07a205dc-1d66-4145-c3b8-e6f5c887598a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predictions: [[0.86004251 0.17083747]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **2**\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "66fvspxB-GAs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class NeuralNetwork:\n",
        "    def __init__(self, input_size, hidden_size, output_size, activation_function='sigmoid', learning_rate=0.01):\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "        self.weights_input_hidden = np.random.randn(hidden_size, input_size)\n",
        "        self.biases_hidden = np.zeros((hidden_size, 1))\n",
        "        self.weights_hidden_output = np.random.randn(output_size, hidden_size)\n",
        "        self.biases_output = np.zeros((output_size, 1))\n",
        "        self.activation_function = activation_function\n",
        "        self.learning_rate = learning_rate\n",
        "\n",
        "    def _sigmoid(self, z):\n",
        "        return 1 / (1 + np.exp(-z))\n",
        "\n",
        "    def _sigmoid_derivative(self, z):\n",
        "        return self._sigmoid(z) * (1 - self._sigmoid(z))\n",
        "\n",
        "    def _relu(self, z):\n",
        "        return np.maximum(0, z)\n",
        "\n",
        "    def _relu_derivative(self, z):\n",
        "        return np.where(z > 0, 1, 0)\n",
        "\n",
        "    def _forward_propagation(self, X):\n",
        "        self.hidden_layer_input = np.dot(self.weights_input_hidden, X) + self.biases_hidden\n",
        "        self.hidden_layer_output = self._sigmoid(self.hidden_layer_input) if self.activation_function == 'sigmoid' else self._relu(self.hidden_layer_input)\n",
        "        self.output_layer_input = np.dot(self.weights_hidden_output, self.hidden_layer_output) + self.biases_output\n",
        "        self.output_layer_output = self._sigmoid(self.output_layer_input)\n",
        "\n",
        "        return self.output_layer_output\n",
        "\n",
        "    def _backward_propagation(self, X, y):\n",
        "        m = X.shape[1]\n",
        "\n",
        "        # Calculate output layer gradients\n",
        "        output_layer_error = self.output_layer_output - y\n",
        "        output_layer_delta = output_layer_error * self._sigmoid_derivative(self.output_layer_input)\n",
        "\n",
        "        # Update output layer weights and biases\n",
        "        self.weights_hidden_output -= self.learning_rate * np.dot(output_layer_delta, self.hidden_layer_output.T) / m\n",
        "        self.biases_output -= self.learning_rate * np.sum(output_layer_delta, axis=1, keepdims=True) / m\n",
        "\n",
        "        # Calculate hidden layer gradients\n",
        "        hidden_layer_error = np.dot(self.weights_hidden_output.T, output_layer_delta)\n",
        "        hidden_layer_delta = hidden_layer_error * (self._sigmoid_derivative(self.hidden_layer_input) if self.activation_function == 'sigmoid' else self._relu_derivative(self.hidden_layer_input))\n",
        "\n",
        "        # Update hidden layer weights and biases\n",
        "        self.weights_input_hidden -= self.learning_rate * np.dot(hidden_layer_delta, X.T) / m\n",
        "        self.biases_hidden -= self.learning_rate * np.sum(hidden_layer_delta, axis=1, keepdims=True) / m\n",
        "\n",
        "    def train(self, X, y, epochs=1000):\n",
        "        for epoch in range(epochs):\n",
        "            for i in range(X.shape[1]):\n",
        "                input_data = X[:, i].reshape(-1, 1)\n",
        "                target_output = y[:, i].reshape(-1, 1)\n",
        "\n",
        "                self._forward_propagation(input_data)\n",
        "                self._backward_propagation(input_data, target_output)\n",
        "\n",
        "    def predict(self, X):\n",
        "        predictions = []\n",
        "        for i in range(X.shape[1]):\n",
        "            input_data = X[:, i].reshape(-1, 1)\n",
        "            output = self._forward_propagation(input_data)\n",
        "            predictions.append(output)\n",
        "\n",
        "        return np.hstack(predictions)\n",
        "\n",
        "# Download MNIST data\n",
        "from keras.datasets import mnist\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "\n",
        "# Resize data to (number of samples, image dimensions)\n",
        "X_train = X_train.reshape((X_train.shape[0], -1)).T / 255.0\n",
        "X_test = X_test.reshape((X_test.shape[0], -1)).T / 255.0\n",
        "\n",
        "# Convert tags to one-hot encoding\n",
        "y_train_one_hot = np.eye(10)[y_train].T\n",
        "y_test_one_hot = np.eye(10)[y_test].T\n",
        "\n",
        "# Determining the network architecture (number of nodes in each layer)\n",
        "input_size = X_train.shape[0]\n",
        "hidden_size = 64\n",
        "output_size = 10\n",
        "\n",
        "# Building the network with the number of nodes in each layer, the activity function and the specified learning rate\n",
        "nn = NeuralNetwork(input_size, hidden_size, output_size, activation_function='sigmoid', learning_rate=0.1)\n",
        "\n",
        "# Network training with training data\n",
        "nn.train(X_train, y_train_one_hot, epochs=5)\n",
        "\n",
        "# Output prediction for test data\n",
        "predictions = nn.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = np.sum(np.argmax(predictions, axis=0) == y_test) / y_test.shape[0]\n",
        "print(\"Test Accuracy:\", accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RVWKT134cxLl",
        "outputId": "5330e952-a710-46c2-b576-2610f49de59c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11490434/11490434 [==============================] - 0s 0us/step\n",
            "Test Accuracy: 0.9334\n"
          ]
        }
      ]
    }
  ]
}